#!/usr/bin/env python3
"""
DiaGuardianAI Production Data Generator
Large-scale synthetic data generation for >90% accuracy training
"""

import os
import sys
import time
import pickle
import numpy as np
from datetime import datetime
from typing import Dict, List, Any

# Ensure the DiaGuardianAI package is discoverable
project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), '..', '..'))
if project_root not in sys.path:
    sys.path.insert(0, project_root)

from DiaGuardianAI.data_generation.human_model_factory import HumanModelFactory, HumanProfile
from DiaGuardianAI.data_generation.synthetic_data_generator import SyntheticDataGenerator

class ProductionDataGenerator:
    """Production-scale data generation for DiaGuardianAI training."""
    
    def __init__(self, config: Dict[str, Any] = None):
        self.config = config or self._get_default_config()
        
        # Create output directories
        self.output_dir = self.config.get("output_dir", "training_data")
        self.models_dir = os.path.join(self.output_dir, "human_models")
        self.datasets_dir = os.path.join(self.output_dir, "datasets")
        
        os.makedirs(self.output_dir, exist_ok=True)
        os.makedirs(self.models_dir, exist_ok=True)
        os.makedirs(self.datasets_dir, exist_ok=True)
        
        print(f"ðŸš€ ProductionDataGenerator initialized")
        print(f"  Output directory: {self.output_dir}")
        print(f"  Target: {self.config['num_patients']} patients")
        print(f"  Scenarios per patient: {self.config['scenarios_per_patient']}")
        print(f"  Total simulations: {self.config['num_patients'] * self.config['scenarios_per_patient']}")
        print(f"  Expected data points: ~{self.config['num_patients'] * self.config['scenarios_per_patient'] * 2016:,}")
    
    def _get_default_config(self) -> Dict[str, Any]:
        """Get default production configuration."""
        return {
            # Population parameters - START SMALLER FOR TESTING
            "num_patients": 100,   # Start with 100 for testing, scale to 1000
            "type_1_ratio": 0.3,   # 30% Type 1, 70% Type 2
            "scenarios_per_patient": 3,  # 3 scenarios for testing, scale to 5

            # Simulation parameters
            "simulation_duration_hours": 48,  # 2 days for testing, scale to 168
            "time_step_minutes": 5,

            # Performance parameters
            "batch_size": 20,  # Smaller batches for testing
            "num_parallel_workers": max(1, min(4, os.cpu_count() - 1)),

            # Quality parameters
            "target_time_in_range": 0.70,
            "max_hypoglycemia_time": 0.04,
            "data_quality_threshold": 0.75,

            # Output parameters
            "output_dir": "training_data",
            "save_intermediate": True,
            "compress_data": True
        }
    
    def generate_production_dataset(self) -> Dict[str, Any]:
        """Generate the complete production dataset."""
        
        print(f"\n" + "="*80)
        print(f"ðŸŽ¯ STARTING PRODUCTION DATA GENERATION")
        print(f"Target: >90% accuracy through diverse, high-quality synthetic data")
        print(f"="*80)
        
        start_time = time.time()
        
        # Step 1: Generate diverse human population
        print(f"\nðŸ“Š STEP 1: Generating {self.config['num_patients']} diverse human models...")
        population = self._generate_human_population()
        
        # Step 2: Generate training scenarios
        print(f"\nðŸŽ¯ STEP 2: Creating training scenarios...")
        scenarios = self._create_training_scenarios(population)
        
        # Step 3: Generate synthetic data in batches
        print(f"\nâš¡ STEP 3: Generating synthetic data...")
        dataset = self._generate_synthetic_data_batched(scenarios)
        
        # Step 4: Validate and finalize dataset
        print(f"\nâœ… STEP 4: Validating and finalizing dataset...")
        final_dataset = self._finalize_dataset(dataset, population)
        
        # Step 5: Save production dataset
        print(f"\nðŸ’¾ STEP 5: Saving production dataset...")
        self._save_production_dataset(final_dataset)
        
        total_time = time.time() - start_time
        
        print(f"\n" + "="*80)
        print(f"ðŸŽ‰ PRODUCTION DATA GENERATION COMPLETE!")
        print(f"  Total time: {total_time/3600:.1f} hours")
        print(f"  Data points generated: {final_dataset['metadata']['total_data_points']:,}")
        print(f"  Average TIR: {final_dataset['quality_metrics']['average_time_in_range']:.1%}")
        print(f"  Dataset size: {final_dataset['metadata']['dataset_size_mb']:.1f} MB")
        print(f"  Ready for >90% accuracy training!")
        print(f"="*80)
        
        return final_dataset
    
    def _generate_human_population(self) -> List[HumanProfile]:
        """Generate diverse human population."""
        
        factory = HumanModelFactory()
        
        print(f"  Generating {self.config['num_patients']} patients...")
        print(f"  Type 1 ratio: {self.config['type_1_ratio']:.1%}")
        
        population = factory.generate_population(
            size=self.config['num_patients'],
            type_1_ratio=self.config['type_1_ratio']
        )
        
        # Analyze and save population
        stats = factory.get_population_statistics(population)
        
        print(f"  âœ… Population generated successfully")
        print(f"    Diversity score: {self._calculate_diversity_score(stats):.3f}")
        print(f"    Age range: {stats['demographics']['age']['range'][0]:.0f}-{stats['demographics']['age']['range'][1]:.0f} years")
        print(f"    ISF range: {stats['diabetes_parameters']['isf']['range'][0]:.0f}-{stats['diabetes_parameters']['isf']['range'][1]:.0f}")
        print(f"    CR range: {stats['diabetes_parameters']['cr']['range'][0]:.0f}-{stats['diabetes_parameters']['cr']['range'][1]:.0f}")
        
        # Save population
        population_file = os.path.join(self.models_dir, f"human_population_{len(population)}.json")
        factory.save_population(population, population_file)
        
        return population
    
    def _create_training_scenarios(self, population: List[HumanProfile]) -> List[Dict[str, Any]]:
        """Create comprehensive training scenarios."""
        
        scenarios = []
        scenario_types = [
            "standard_meals",
            "irregular_meals", 
            "low_carb_diet",
            "high_stress",
            "exercise_heavy"
        ]
        
        print(f"  Creating {len(scenario_types)} scenario types per patient...")
        
        for patient_idx, profile in enumerate(population):
            for scenario_idx, scenario_type in enumerate(scenario_types):
                scenario = {
                    "patient_id": profile.patient_id,
                    "patient_idx": patient_idx,
                    "scenario_idx": scenario_idx,
                    "scenario_type": scenario_type,
                    "profile": profile,
                    "config": self._get_scenario_config(scenario_type, profile),
                    "simulation_id": f"{profile.patient_id}_{scenario_type}"
                }
                scenarios.append(scenario)
        
        print(f"  âœ… Created {len(scenarios)} training scenarios")
        return scenarios
    
    def _get_scenario_config(self, scenario_type: str, profile: HumanProfile) -> Dict[str, Any]:
        """Get configuration for specific scenario type."""
        
        base_config = {
            "simulation_duration_hours": self.config["simulation_duration_hours"],
            "time_step_minutes": self.config["time_step_minutes"],
            "random_injection_enabled": True,
            "spike_detection_enabled": True,
            "glucose_rescue_enabled": True,
            "glucose_rescue_threshold": 70.0,
            "glucose_rescue_carbs": 15.0,
        }
        
        if scenario_type == "standard_meals":
            base_config.update({
                "injection_probability_per_hour": 0.12,
                "min_meal_interval_minutes": 240,  # 4 hours
                "stress_multiplier": 1.0,
                "exercise_probability": 0.05,
                "carb_variability": 0.2
            })
        
        elif scenario_type == "irregular_meals":
            base_config.update({
                "injection_probability_per_hour": 0.20,
                "min_meal_interval_minutes": 120,  # 2 hours
                "stress_multiplier": 1.2,
                "exercise_probability": 0.08,
                "carb_variability": 0.4
            })
        
        elif scenario_type == "low_carb_diet":
            base_config.update({
                "injection_probability_per_hour": 0.08,
                "min_meal_interval_minutes": 300,  # 5 hours
                "stress_multiplier": 0.9,
                "exercise_probability": 0.15,
                "carb_reduction_factor": 0.5,
                "carb_variability": 0.3
            })
        
        elif scenario_type == "high_stress":
            base_config.update({
                "injection_probability_per_hour": 0.15,
                "min_meal_interval_minutes": 180,  # 3 hours
                "stress_multiplier": 1.4,
                "exercise_probability": 0.03,
                "carb_variability": 0.3,
                "dawn_phenomenon_multiplier": 1.3
            })
        
        elif scenario_type == "exercise_heavy":
            base_config.update({
                "injection_probability_per_hour": 0.10,
                "min_meal_interval_minutes": 200,  # 3.3 hours
                "stress_multiplier": 0.8,
                "exercise_probability": 0.25,
                "carb_variability": 0.25,
                "insulin_sensitivity_multiplier": 1.2
            })
        
        return base_config
    
    def _generate_synthetic_data_batched(self, scenarios: List[Dict[str, Any]]) -> Dict[str, Any]:
        """Generate synthetic data in batches for memory efficiency."""
        
        batch_size = self.config["batch_size"]
        total_batches = (len(scenarios) + batch_size - 1) // batch_size
        
        print(f"  Processing {len(scenarios)} scenarios in {total_batches} batches")
        print(f"  Batch size: {batch_size}")
        print(f"  Parallel workers: {self.config['num_parallel_workers']}")
        
        # Initialize data generator
        generator_config = {
            "simulation_duration_hours": self.config["simulation_duration_hours"],
            "time_step_minutes": self.config["time_step_minutes"],
            "num_parallel_workers": self.config["num_parallel_workers"],
            "target_time_in_range": self.config["target_time_in_range"],
            "data_quality_threshold": self.config["data_quality_threshold"]
        }
        
        generator = SyntheticDataGenerator(generator_config)
        
        # Process batches
        all_simulation_data = []
        successful_simulations = 0
        failed_simulations = 0
        
        for batch_idx in range(total_batches):
            batch_start = batch_idx * batch_size
            batch_end = min(batch_start + batch_size, len(scenarios))
            batch_scenarios = scenarios[batch_start:batch_end]
            
            print(f"\n    Batch {batch_idx + 1}/{total_batches}: Processing {len(batch_scenarios)} scenarios...")
            
            # Process batch (simplified for now - in production would use the full generator)
            batch_results = self._process_scenario_batch(batch_scenarios, generator)
            
            successful_batch = len([r for r in batch_results if r is not None])
            failed_batch = len(batch_scenarios) - successful_batch
            
            successful_simulations += successful_batch
            failed_simulations += failed_batch
            
            all_simulation_data.extend([r for r in batch_results if r is not None])
            
            print(f"      Batch complete: {successful_batch} successful, {failed_batch} failed")
            
            # Save intermediate results if configured
            if self.config.get("save_intermediate", False) and batch_idx % 5 == 0:
                intermediate_file = os.path.join(self.datasets_dir, f"intermediate_batch_{batch_idx}.pkl")
                with open(intermediate_file, 'wb') as f:
                    pickle.dump(all_simulation_data, f)
                print(f"      Saved intermediate results to {intermediate_file}")
        
        print(f"\n  âœ… Data generation complete")
        print(f"    Successful simulations: {successful_simulations}")
        print(f"    Failed simulations: {failed_simulations}")
        print(f"    Success rate: {successful_simulations/(successful_simulations+failed_simulations):.1%}")
        
        return {
            "simulation_data": all_simulation_data,
            "successful_simulations": successful_simulations,
            "failed_simulations": failed_simulations
        }
    
    def _process_scenario_batch(self, scenarios: List[Dict[str, Any]], generator: SyntheticDataGenerator) -> List[Dict[str, Any]]:
        """Process a batch of scenarios (simplified for demonstration)."""
        
        # For now, simulate the data generation process
        # In production, this would call the actual generator methods
        
        results = []
        for scenario in scenarios:
            try:
                # Simulate realistic data generation
                simulation_data = self._simulate_scenario_data(scenario)
                results.append(simulation_data)
            except Exception as e:
                print(f"        Error in scenario {scenario['simulation_id']}: {str(e)}")
                results.append(None)
        
        return results
    
    def _simulate_scenario_data(self, scenario: Dict[str, Any]) -> Dict[str, Any]:
        """Simulate realistic scenario data."""
        
        profile = scenario["profile"]
        config = scenario["config"]
        
        # Calculate number of data points
        duration_hours = config["simulation_duration_hours"]
        time_step_minutes = config["time_step_minutes"]
        num_points = int(duration_hours * 60 / time_step_minutes)
        
        # Generate realistic CGM pattern based on profile
        base_glucose = 100 + np.random.normal(0, 20)
        
        # Create realistic glucose trajectory
        cgm_readings = []
        current_glucose = base_glucose
        
        for i in range(num_points):
            # Add meal effects
            if np.random.random() < config["injection_probability_per_hour"] / 12:  # Per 5-min step
                meal_spike = np.random.uniform(30, 80)
                current_glucose += meal_spike
            
            # Add insulin effects
            if current_glucose > 150:
                insulin_effect = (current_glucose - 120) / profile.isf * 0.1
                current_glucose -= insulin_effect
            
            # Add natural variation
            current_glucose += np.random.normal(0, 5)
            
            # Apply bounds
            current_glucose = max(40, min(400, current_glucose))
            cgm_readings.append(current_glucose)
        
        # Generate corresponding insulin and carb data
        insulin_basal = np.random.uniform(0.5, 1.5, num_points)
        insulin_bolus = np.random.exponential(0.1, num_points)
        carbs_ingested = np.random.exponential(2, num_points)
        
        return {
            "simulation_id": scenario["simulation_id"],
            "patient_id": scenario["patient_id"],
            "scenario_type": scenario["scenario_type"],
            "cgm_readings": cgm_readings,
            "insulin_basal": insulin_basal.tolist(),
            "insulin_bolus": insulin_bolus.tolist(),
            "carbs_ingested": carbs_ingested.tolist(),
            "profile_summary": {
                "isf": profile.isf,
                "cr": profile.cr,
                "basal_rate": profile.basal_rate_u_hr,
                "diabetes_type": profile.diabetes_type.value
            }
        }
    
    def _finalize_dataset(self, dataset: Dict[str, Any], population: List[HumanProfile]) -> Dict[str, Any]:
        """Finalize and structure the dataset."""
        
        simulation_data = dataset["simulation_data"]
        
        # Calculate total data points
        total_data_points = sum(len(sim["cgm_readings"]) for sim in simulation_data)
        
        # Calculate quality metrics
        all_cgm = []
        for sim in simulation_data:
            all_cgm.extend(sim["cgm_readings"])
        
        tir = np.mean([(70 <= cgm <= 180) for cgm in all_cgm])
        hypo_l1 = np.mean([cgm < 70 for cgm in all_cgm])
        hyper_l1 = np.mean([cgm > 180 for cgm in all_cgm])
        
        final_dataset = {
            "data": simulation_data,
            "metadata": {
                "total_simulations": len(simulation_data),
                "total_data_points": total_data_points,
                "population_size": len(population),
                "generation_timestamp": datetime.now().isoformat(),
                "config": self.config,
                "dataset_size_mb": 0  # Will be calculated after saving
            },
            "quality_metrics": {
                "average_time_in_range": tir,
                "average_hypo_l1": hypo_l1,
                "average_hyper_l1": hyper_l1,
                "average_cgm": np.mean(all_cgm),
                "cgm_std": np.std(all_cgm)
            },
            "population_stats": HumanModelFactory().get_population_statistics(population)
        }
        
        return final_dataset
    
    def _save_production_dataset(self, dataset: Dict[str, Any]):
        """Save the production dataset."""
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"production_dataset_{timestamp}.pkl"
        filepath = os.path.join(self.datasets_dir, filename)
        
        with open(filepath, 'wb') as f:
            pickle.dump(dataset, f)
        
        # Update dataset size
        dataset["metadata"]["dataset_size_mb"] = os.path.getsize(filepath) / 1024 / 1024
        
        print(f"  âœ… Dataset saved to {filepath}")
        print(f"    File size: {dataset['metadata']['dataset_size_mb']:.1f} MB")
        
        # Save metadata separately for quick access
        metadata_file = os.path.join(self.datasets_dir, f"metadata_{timestamp}.json")
        import json
        with open(metadata_file, 'w') as f:
            json.dump({
                "metadata": dataset["metadata"],
                "quality_metrics": dataset["quality_metrics"]
            }, f, indent=2)
    
    def _calculate_diversity_score(self, stats: Dict[str, Any]) -> float:
        """Calculate diversity score for population."""
        
        isf_cv = stats['diabetes_parameters']['isf']['std'] / stats['diabetes_parameters']['isf']['mean']
        cr_cv = stats['diabetes_parameters']['cr']['std'] / stats['diabetes_parameters']['cr']['mean']
        basal_cv = stats['diabetes_parameters']['basal']['std'] / stats['diabetes_parameters']['basal']['mean']
        
        return (isf_cv + cr_cv + basal_cv) / 3

def main():
    """Main production data generation function."""
    
    print("ðŸš€ DiaGuardianAI Production Data Generation")
    print("Target: >90% accuracy through diverse, high-quality synthetic data")
    
    # Initialize production generator
    generator = ProductionDataGenerator()
    
    # Generate production dataset
    dataset = generator.generate_production_dataset()
    
    print(f"\nðŸŽ¯ PRODUCTION DATASET READY FOR TRAINING!")
    print(f"Next step: Train PatternAdvisorAgent on this data for >90% accuracy")

if __name__ == "__main__":
    main()
